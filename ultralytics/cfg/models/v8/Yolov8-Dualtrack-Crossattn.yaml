nc: 2 # number of classes
scales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'
  # [depth, width, max_channels]
  # n: [0.33, 0.25, 1024] # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs
  # s: [0.33, 0.50, 1024] # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs
  # m: [0.67, 0.75, 768] # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs
  # l: [1.00, 1.00, 512] # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs
  # x: [1.00, 1.25, 512] # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs
backbone:
    #Input image placeholder
  - [-1, 1, nn.Identity, []] #0
  
  # CNN track
  - [0, 1, TorchVision, [960, mobilenet_v3_large, "DEFAULT", True, 2, True]]  # 1
  - [-1, 1, Index, [40, 7]] #2 multiscale 1
  - [1, 1, Index, [112, 13]] #3 multicale 2
  - [1, 1, Index, [960, 17]] #4
  - [-1, 1, SPPF, [960, 5]] #5 multiscale 3
  
  # Swin track
  - [0, 1, TorchVision, [768, swin_t, DEFAULT, True, 4, True]] #6
  - [-1, 1, Index, [192 ,4]] #7
  - [-1, 1, torchvision.ops.Permute, [[0, 3, 1, 2]]] #8 multiscale 1
  - [6, 1, Index, [384, 6]] #9
  - [-1, 1, torchvision.ops.Permute, [[0, 3, 1, 2]]] #10 multiscale 2
  - [6, 1, Index, [768, 9]] #11
  - [-1, 1, torchvision.ops.Permute, [[0, 3, 1, 2]]] #12
  - [-1, 1, SPPF, [768, 5]] #13 multiscale 3

  # Prepare CNN features for Attention
  - [2, 1, Conv, [64, 1]]  # 14 Reduce CNN channels for scale 1
  - [-1, 1, torchvision.ops.Permute, [[0, 2, 3, 1]]] # 15 B, H, W, C
  - [-1, 1, nn.Flatten, [1, 2]]      # 16 Flatten H and W dimensions (B, H*W, C)

  # Prepare Swin features for Attention
  - [8, 1, torchvision.ops.Permute, [[0, 2, 3, 1]]] # 17 B, H, W, C
  - [-1, 1, nn.Flatten, [1, 2]]      # 18 Flatten H and W dimensions (B, H*W, C)

  # Cross-Attention Modules (Swin attends to CNN)
  - [[18, 16, 16], 1, MultiHeadAttention, [192, 8, 64]]  # 19 Swin (Q) attends to CNN (K, V)

  # Reshape the attended features back to the original shape
  - [19, 1, nn.Linear, [192, 192]] # 20 Transform attended swin features
  - [-1, 1, nn.Unflatten, [1, [80, 80]]] # 21 Unflatten back to (B, 80, 80, 192)
  - [-1, 1, torchvision.ops.Permute, [[0, 3, 1, 2]]]  # 22: Permute back to (B, 192, 80, 80)

  # Prepare CNN features for Attention
  - [3, 1, Conv, [128, 1]]  # 23 Reduce CNN channels for scale 2
  - [-1, 1, torchvision.ops.Permute, [[0, 2, 3, 1]]] # 24 B, H, W, C
  - [-1, 1, nn.Flatten, [1, 2]]      # 25 Flatten H and W dimensions (B, H*W, C)

  # Prepare Swin features for Attention
  - [10, 1, torchvision.ops.Permute, [[0, 2, 3, 1]]] # 26 B, H, W, C
  - [-1, 1, nn.Flatten, [1, 2]]      # 27 Flatten H and W dimensions (B, H*W, C)

  # Cross-Attention Modules (Swin attends to CNN)
  - [[27, 25, 25], 1, MultiHeadAttention, [384, 8, 128]]  # 28 Swin (Q) attends to CNN (K, V)

  # Reshape the attended features back to the original shape
  - [28, 1, nn.Linear, [384, 384]] # 29 Transform attended swin features
  - [-1, 1, nn.Unflatten, [1, [40, 40]]] # 30 Unflatten back to (B, 40, 40, 384)
  - [-1, 1, torchvision.ops.Permute, [[0, 3, 1, 2]]]  # 31: Permute back to (B, 384, 40, 40)

  # Prepare CNN features for Attention
  - [5, 1, Conv, [256, 1]]  # 32 Reduce CNN channels for scale 3
  - [-1, 1, torchvision.ops.Permute, [[0, 2, 3, 1]]] # 33 B, H, W, C
  - [-1, 1, nn.Flatten, [1, 2]]      # 34 Flatten H and W dimensions (B, H*W, C)

  # Prepare Swin features for Attention
  - [12, 1, torchvision.ops.Permute, [[0, 2, 3, 1]]] # 35 B, H, W, C
  - [-1, 1, nn.Flatten, [1, 2]]      # 36 Flatten H and W dimensions (B, H*W, C)

  # Cross-Attention Modules (Swin attends to CNN)
  - [[36, 34, 34], 1, MultiHeadAttention, [768, 8, 256]]  # 37 Swin (Q) attends to CNN (K, V)

  # Reshape the attended features back to the original shape
  - [37, 1, nn.Linear, [768, 768]] # 38 Transform attended swin features
  - [-1, 1, nn.Unflatten, [1, [20, 20]]] # 39 Unflatten back to (B, 20, 20, 768)
  - [-1, 1, torchvision.ops.Permute, [[0, 3, 1, 2]]]  # 40: Permute back to (B, 768, 20, 20)

  # Channel reduction before concatenation
  - [22, 1, Conv, [128, 1]]  # 41 Reduce attention features scale 1
  - [2, 1, Conv, [128, 1]]   # 42 Reduce CNN features scale 1
  - [[41, 42], 1, Concat, [1]] # 43 concat scale 1 (256 channels)

  - [31, 1, Conv, [256, 1]]  # 44 Reduce attention features scale 2
  - [3, 1, Conv, [256, 1]]   # 45 Reduce CNN features scale 2
  - [[44, 45], 1, Concat, [1]] # 46 concat scale 2 (512 channels)

  - [40, 1, Conv, [512, 1]]  # 47 Reduce attention features scale 3
  - [5, 1, Conv, [512, 1]]   # 48 Reduce CNN features scale 3
  - [[47, 48], 1, Concat, [1]] # 49 concat scale 3 (1024 channels)

head:
  - [49, 1, nn.Upsample, [None, 2, "nearest"]] #50 upsample scale 3 to match scale 2
  - [[-1, 46], 1, Concat, [1]] # 51 concat upsampled scale 3 with scale 2
  - [-1, 1, C2f, [512]] #52 process combined features

  - [52, 1, nn.Upsample, [None, 2, "nearest"]] #53 upsample to match scale 1
  - [[-1, 43], 1, Concat, [1]] # 54 concat with scale 1
  - [-1, 1, C2f, [256]] #55 process combined features

  # Bottom-Up Pathway (from Scale 1 to Scale 3)
  - [55, 1, Conv, [256, 3, 2]]    # 56: Downsample to scale 2
  - [[-1, 52], 1, Concat, [1]]   # 57: Concatenate with scale 2 features
  - [-1, 1, C2f, [512, True]]    # 58: Process combined features

  - [58, 1, Conv, [512, 3, 2]]    # 59: Downsample to scale 3
  - [[-1, 49], 1, Concat, [1]]   # 60: Concatenate with scale 3 features
  - [-1, 1, C2f, [1024, True]]   # 61: Process final features

  - [[55, 58, 61], 1, Detect, [nc]]  # 62: Detect(P3, P4, P5)
  
