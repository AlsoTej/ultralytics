# nc: 2 # number of classes
# scales:
#   s: [1.0, 1.0, 1536]

# backbone:
#   - [-1, 1, nn.Identity, []] # 0

#   # CNN (ConvNeXt Tiny)
#   - [0, 1, TorchVision, [768, "convnext_tiny", "DEFAULT", True, 2, True, False]]  # 1
#   - [-1, 1, Index, [192, 4]]  # extracts 4th output (1, 192, 80, 80) - 2
#   - [1, 1, Index, [384, 6]]  # extracts 6th output (1, 384, 40, 40) - 3
#   - [1, 1, Index, [768, 8]]  # extracts 8th output (1, 768, 20, 20) - 4
#   - [-1, 1, SPPF, [768, 3]]  # 5: Add SPPF for ConvNeXt (768 channels, kernel size 5)

#   # Swin Transformer
#   - [0, 1, TorchVision, [768, swin_v2_t, DEFAULT, True, 4, True, True]] # 6
#   - [-1, 1, Index, [192, 4]] # 7
#   - [-1, 1, torchvision.ops.Permute, [[0, 3, 1, 2]]] # 8 (multiscale 1)
#   - [6, 1, Index, [384, 6]] # 9
#   - [-1, 1, torchvision.ops.Permute, [[0, 3, 1, 2]]] # 10 (multiscale 2)
#   - [6, 1, Index, [768, 9]] # 11
#   - [-1, 1, torchvision.ops.Permute, [[0, 3, 1, 2]]] # 12 (multiscale 3)
#   - [-1, 1, SPPF, [768, 3]]  # 13: Add SPPF for Swin Transformer (768 channels, kernel size 5)

#   # Concat features along channel dimension
#   - [[2, 8], 1, Concat, [1]] # 14 concat scale 1 (192 + 192 = 384 channels)
#   - [-1, 1, EMA, [384]]      # 15 EMA for scale 1

#   - [[3, 10], 1, Concat, [1]] # 16 concat scale 2 (384 + 384 = 768 channels)
#   - [-1, 1, EMA, [768]]      # 17 EMA for scale 2

#   - [[5, 13], 1, Concat, [1]] # 18 concat scale 3 (768 + 768 = 1536 channels)
#   - [-1, 1, EMA, [1536]]      # 19 EMA for scale 3

# head:
#   # Top-Down Pathway
#   - [19, 1, nn.Upsample, [None, 2, "nearest"]] # 20 upsample EMA3 output to match H and W with EMA2
#   - [[-1, 17], 1, BiFPN_Concat2, [1]] # 21 concat EMA3 + EMA2 (1536 + 768 = 2304 channels)
#   - [-1, 1, C3k2, [512, False]] # 22 reduce channels to 512

#   - [22, 1, nn.Upsample, [None, 2, "nearest"]] # 23 upsample to match H and W with EMA1
#   - [[-1, 15], 1, BiFPN_Concat2, [1]] # 24 concat with EMA1 (512 + 384 = 896 channels)
#   - [-1, 1, C3k2, [256, False]] # 25 reduce channels to 256

#   # Bottom-Up Pathway
#   - [25, 1, DWConv, [256, 3, 2]]    # 26: Downsample
#   - [[-1, 22], 1, BiFPN_Concat2, [1]]    # 27: BiFPN Concatenate (256 + 512 = 768 channels)
#   - [-1, 1, C3k2, [512, True]]     # 28: C2f (reduce to 512 channels)

#   - [28, 1, DWConv, [512, 3, 2]]    # 29: Downsample
#   - [[-1, 19], 1, BiFPN_Concat2, [1]]    # 30: Concatenate with EMA3 (512 + 1536 = 2048 channels)
#   - [-1, 1, C3k2, [1024, True]]    # 31: C2f (reduce to 1024 channels)

#   # Detection
#   - [[25, 28, 31], 1, RTDETRDecoder, [nc]]  # 32: Detect(P3, P4, P5)



nc: 2  # number of classes
scales:
  s: [1.0, 1.0, 1536]  # Note: Not using dynamic scaling, kept for reference

backbone:
  - [-1, 1, nn.Identity, []]  # 0

  # CNN (ConvNeXt Tiny) - Fine-tuned from pre-trained weights
  - [0, 1, TorchVision, [768, "convnext_tiny", "DEFAULT", True, 2, True, False]]  # 1
  - [-1, 1, Index, [192, 4]]  # extracts 4th output (1, 192, 80, 80) - 2
  - [1, 1, Index, [384, 6]]  # extracts 6th output (1, 384, 40, 40) - 3
  - [1, 1, Index, [768, 8]]  # extracts 8th output (1, 768, 20, 20) - 4
  - [-1, 1, SPPF, [512, 3]]  # 5: Fixed to [c2, k], reduced channels from 768 to 512

  # Swin Transformer - Pre-trained and frozen
  - [0, 1, TorchVision, [768, "swin_v2_t", "DEFAULT", True, 4, True, True]]  # 6
  - [-1, 1, Index, [192, 4]]  # 7
  - [-1, 1, torchvision.ops.Permute, [[0, 3, 1, 2]]]  # 8 (multiscale 1)
  - [6, 1, Index, [384, 6]]  # 9
  - [-1, 1, torchvision.ops.Permute, [[0, 3, 1, 2]]]  # 10 (multiscale 2)
  - [6, 1, Index, [768, 9]]  # 11
  - [-1, 1, torchvision.ops.Permute, [[0, 3, 1, 2]]]  # 12 (multiscale 3)
  - [-1, 1, SPPF, [512, 3]]  # 13: Fixed to [c2, k], reduced channels from 768 to 512

  # Concat features along channel dimension
  - [[2, 8], 1, Concat, [1]]  # 14 concat scale 1 (192 + 192 = 384 channels)
  - [-1, 1, EMA, [320, 320]]  # 15: Reduced channels from 384 to 320
  - [[3, 10], 1, Concat, [1]]  # 16 concat scale 2 (384 + 384 = 768 channels)
  - [-1, 1, EMA, [640, 640]]  # 17: Reduced channels from 768 to 640
  - [[5, 13], 1, Concat, [1]]  # 18 concat scale 3 (512 + 512 = 1024 channels)
  - [-1, 1, EMA, [1280, 1280]]  # 19: Reduced channels from 1536 to 1280

head:
  # Top-Down Pathway
  - [19, 1, nn.Upsample, [None, 2, "nearest"]]  # 20 upsample EMA3 output to match H and W with EMA2
  - [[-1, 17], 1, BiFPN_Concat2, [1]]  # 21 concat EMA3 + EMA2 (1280 + 640 = 1920 channels)
  - [-1, 1, C3k2, [1920, 384, 1, False]]  # 22: Reduced output channels from 512 to 384
  - [22, 1, nn.Upsample, [None, 2, "nearest"]]  # 23 upsample to match H and W with EMA1
  - [[-1, 15], 1, BiFPN_Concat2, [1]]  # 24 concat with EMA1 (384 + 320 = 704 channels)
  - [-1, 1, C3k2, [704, 192, 1, False]]  # 25: Reduced output channels from 256 to 192

  # Bottom-Up Pathway
  - [25, 1, DWConv, [192, 192, 3, 2]]  # 26: Reduced channels from 256 to 192
  - [[-1, 22], 1, BiFPN_Concat2, [1]]  # 27: Concatenate (192 + 384 = 576 channels)
  - [-1, 1, C3k2, [576, 384, 1, True]]  # 28: Reduced output channels from 512 to 384
  - [28, 1, DWConv, [384, 384, 3, 2]]  # 29: Reduced channels from 512 to 384
  - [[-1, 19], 1, BiFPN_Concat2, [1]]  # 30: Concatenate with EMA3 (384 + 1280 = 1664 channels)
  - [-1, 1, C3k2, [1664, 768, 1, True]]  # 31: Reduced output channels from 1024 to 768

  # Detection
  - [[25, 28, 31], 1, RTDETRDecoder, [nc, [192,
